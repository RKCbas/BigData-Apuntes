{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covid-19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una empresa farmacéutica internacional está considerando realizar pruebas clínicas de una nueva vacuna/tratamiento para COVID-19 en México. Para tomar decisiones informadas sobre la ubicación y el alcance de las pruebas, la empresa necesita analizar datos epidemiológicos y demográficos relevantes. Con no menos de 1,000,000 registros.\n",
    "\n",
    "### Adquisición y Limpieza de Datos:\n",
    "\n",
    "   - Identificar y evaluar fuentes de datos confiables sobre COVID-19 en México.\n",
    "   - Descargar y transformar datos en formatos adecuados (CSV, JSON).\n",
    "   - Limpiar y preparar los datos para el análisis. Por ejemplo: fecha (mm-dd-yyyy), no ids, si no nombres.\n",
    "\n",
    "### Ingesta de Datos en Elasticsearch:\n",
    "\n",
    "   - Crear un índice en Elasticsearch para almacenar los datos de COVID-19.\n",
    "   - Definir mappings para optimizar las búsquedas y el análisis.\n",
    "   - Cargar los datos en Elasticsearch utilizando herramientas como Python, Pandas y la librería elasticsearch.\n",
    "   - Definir queries básicas\n",
    "\n",
    "## Solución\n",
    "\n",
    "Para la solución de esta problemática encontramos un dataset de datos abiertos de la dirección de epidemiología de Mexico ([Datos](https://www.gob.mx/salud/documentos/datos-abiertos-152127)) en este dataset vamos a guardar en un df los datos cargados desde el csv y después vamos a modificar los datos para ponerlos en el formato solicitado para después guardarlo todo en un json para que después lo podamos subir a elastic search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importar librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ElasticSearchProvider import ElasticSearchProvider\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modificar datos\n",
    "\n",
    "### Rutas de archivos\n",
    "\n",
    "Primeramente vamos a definir variables que se repiten mucho que nos van a ayudar con la dirección de los archivos y en las respuestas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = \"Datos.json\"\n",
    "csv_file_path = \"COVID19MEXICO2020.csv\"\n",
    "excel_catalog_path = \"Catálogos.xlsx\"\n",
    "RESPONSE_LITERAL = \"response: \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Descripción: Debido a que las rutas de los archivos que vamos a utilizar se repiten muchas veces vamos a declarar variables que contengan los nombres de los archivos\n",
    "\n",
    "### Entidades y municipios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un pequeño diccionario que nos va a ayudar a obtener el nombre de la entidad dependiendo de su id\n",
    "ENTIDADES_DICT = {\n",
    "    1: \"AGUASCALIENTES\",\n",
    "    2: \"BAJA CALIFORNIA\",\n",
    "    3: \"BAJA CALIFORNIA SUR\",\n",
    "    4: \"CAMPECHE\",\n",
    "    5: \"COAHUILA DE ZARAGOZA\",\n",
    "    6: \"COLIMA\",\n",
    "    7: \"CHIAPAS\",\n",
    "    8: \"CHIHUAHUA\",\n",
    "    9: \"CIUDAD DE MÉXICO\",\n",
    "    10: \"DURANGO\",\n",
    "    11: \"GUANAJUATO\",\n",
    "    12: \"GUERRERO\",\n",
    "    13: \"HIDALGO\",\n",
    "    14: \"JALISCO\",\n",
    "    15: \"MÉXICO\",\n",
    "    16: \"MICHOACÁN DE OCAMPO\",\n",
    "    17: \"MORELOS\",\n",
    "    18: \"NAYARIT\",\n",
    "    19: \"NUEVO LEÓN\",\n",
    "    20: \"OAXACA\",\n",
    "    21: \"PUEBLA\",\n",
    "    22: \"QUERÉTARO\",\n",
    "    23: \"QUINTANA ROO\",\n",
    "    24: \"SAN LUIS POTOSÍ\",\n",
    "    25: \"SINALOA\",\n",
    "    26: \"SONORA\",\n",
    "    27: \"TABASCO\",\n",
    "    28: \"TAMAULIPAS\",\n",
    "    29: \"TLAXCALA\",\n",
    "    30: \"VERACRUZ DE IGNACIO DE LA LLAVE\",\n",
    "    31: \"YUCATÁN\",\n",
    "    32: \"ZACATECAS\",\n",
    "    36: \"ESTADOS UNIDOS MEXICANOS\",\n",
    "    97: \"NO APLICA\",\n",
    "    98: \"SE IGNORA\",\n",
    "    99: \"NO ESPECIFICADO\"\n",
    "}\n",
    "\n",
    "# Creamos una función que utilizando le entidad_id nos regrese el nombre de la entidad en base al diccionario de arriba\n",
    "def obtener_nombre_entidad(entidad_id) -> str:\n",
    "    \"\"\"\n",
    "    Devuelve el nombre de la entidad correspondiente al ID proporcionado.\n",
    "    Si el ID no está en el diccionario, devuelve el ID original.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return ENTIDADES_DICT.get(entidad_id, entidad_id)\n",
    "    except Exception as e:\n",
    "        print(f\"Error al obtener el nombre de la entidad para ID {entidad_id}: {e}\")\n",
    "        return entidad_id\n",
    "\n",
    "# Cargar los catálogos de entidades y municipios desde el archivo Excel\n",
    "entidades_df = pd.read_excel(excel_catalog_path, sheet_name=\"Catálogo de ENTIDADES\")\n",
    "municipios_df = pd.read_excel(excel_catalog_path, sheet_name=\"Catálogo MUNICIPIOS\")\n",
    "\n",
    "# Renombrar columnas para facilitar el merge\n",
    "entidades_df.rename(columns={\"CLAVE_ENTIDAD\": \"ENTIDAD_RES\", \"ENTIDAD_FEDERATIVA\": \"NOMBRE_ENTIDAD\"}, inplace=True)\n",
    "municipios_df.rename(columns={\"CLAVE_ENTIDAD\": \"ENTIDAD_RES\", \"CLAVE_MUNICIPIO\": \"MUNICIPIO_RES\", \"MUNICIPIO\": \"NOMBRE_MUNICIPIO\"}, inplace=True)\n",
    "\n",
    "# delete ABREVIATURA column from entidades_df debido a que no lo vamos a utilizar\n",
    "entidades_df.drop(columns=[\"ABREVIATURA\"], inplace=True)\n",
    "\n",
    "# Definir el mapeo de tipos de datos para las columnas\n",
    "dtype_mapping = {\n",
    "    \"PAIS_NACIONALIDAD\": \"str\",  # Columna 38\n",
    "    \"PAIS_ORIGEN\": \"str\",       # Columna 39\n",
    "}\n",
    "\n",
    "# cargar el df desde el csv\n",
    "df = pd.read_csv(csv_file_path, dtype=dtype_mapping)\n",
    "\n",
    "# Hacer merge para agregar los nombres de entidad y municipio\n",
    "df = df.merge(entidades_df, on=\"ENTIDAD_RES\", how=\"left\")\n",
    "df = df.merge(municipios_df, on=[\"ENTIDAD_RES\", \"MUNICIPIO_RES\"], how=\"left\")\n",
    "\n",
    "# Crear el campo RES como un diccionario\n",
    "df[\"RES\"] = df.apply(\n",
    "    lambda row: {\n",
    "        \"IS_ENTIDAD\": row[\"ENTIDAD_RES\"],\n",
    "        \"ENTIDAD\": row[\"NOMBRE_ENTIDAD\"],\n",
    "        \"ID_MUNICIPIO\": row[\"MUNICIPIO_RES\"],\n",
    "        \"MUNICIPIO\": row[\"NOMBRE_MUNICIPIO\"]\n",
    "    },\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# delete the columns \"ENTIDAD_RES\", \"MUNICIPIO_RES\", \"NOMBRE_ENTIDAD\", \"NOMBRE_MUNICIPIO\"\n",
    "df.drop(columns=[\"ENTIDAD_RES\", \"MUNICIPIO_RES\", \"NOMBRE_ENTIDAD\", \"NOMBRE_MUNICIPIO\"], inplace=True)\n",
    "\n",
    "# Leave only the first 1000000 rows\n",
    "df = df.head(1000000)\n",
    "\n",
    "# Save the DataFrame to a JSON file\n",
    "df.to_json(json_file_path, orient=\"records\", lines=False, indent=4)\n",
    "print(\"Archivo JSON creado\")\n",
    "\n",
    "# Load the JSON file into a DataFrame\n",
    "df = pd.read_json(json_file_path, orient=\"records\", lines=False)\n",
    "\n",
    "# Change \"ENTIDAD_UM\", \"ENTIDAD_NAC\" to a dictionary of the id and the name of the entity\n",
    "if \"ENTIDAD_UM\" in df.columns:\n",
    "    df[\"ENTIDAD_UM\"] = df[\"ENTIDAD_UM\"].apply(\n",
    "        lambda entidad_id: {\n",
    "            \"ID\": entidad_id,\n",
    "            \"NOMBRE\": obtener_nombre_entidad(int(entidad_id)) if pd.notnull(entidad_id) else None\n",
    "        }\n",
    "    )\n",
    "\n",
    "if \"ENTIDAD_NAC\" in df.columns:\n",
    "    df[\"ENTIDAD_NAC\"] = df[\"ENTIDAD_NAC\"].apply(\n",
    "        lambda entidad_id: {\n",
    "            \"ID\": entidad_id,\n",
    "            \"NOMBRE\": obtener_nombre_entidad(int(entidad_id)) if pd.notnull(entidad_id) else None\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Guardar el DataFrame modificado en el archivo JSON\n",
    "df.to_json(json_file_path, orient=\"records\", lines=False, indent=4)\n",
    "print(\"Archivo JSON actualizado con los cambios en 'ENTIDAD_UM' y 'ENTIDAD_NAC'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Descripción: Con esto cargamos los datos del csv y le hacemos algunas modificaciones básicas donde guardamos unicamente lso primeros 1´000,000 registros y también reorganizamos las entidades para que cuando se guarden en el json se guarde el id y el nombre de la entidad y para la residencia se guarda tanto la entidad como el municipio con el sus id´s, el mapping para estas modificaciones seria algo similar a esto:\n",
    "```json\n",
    "    \"ENTIDAD_UM\": {\n",
    "        \"properties\": {\n",
    "          \"ID\": { \"type\": \"integer\" },\n",
    "          \"NOMBRE\": { \"type\": \"keyword\" }\n",
    "        }\n",
    "      },\n",
    "      \"ENTIDAD_NAC\": {\n",
    "        \"properties\": {\n",
    "          \"ID\": { \"type\": \"integer\" },\n",
    "          \"NOMBRE\": { \"type\": \"keyword\" }\n",
    "        }\n",
    "      },\n",
    "      \"RES\": {\n",
    "        \"properties\": {\n",
    "          \"id_entidad\": { \"type\": \"integer\" },\n",
    "          \"entidad\": { \"type\": \"keyword\" },\n",
    "          \"id_municipio\": { \"type\": \"integer\" },\n",
    "          \"municipio\": { \"type\": \"keyword\" }\n",
    "        }\n",
    "      }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modificar país origen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON file into a DataFrame\n",
    "df = pd.read_json(json_file_path, orient=\"records\", lines=False)\n",
    "\n",
    "# Check and convert 'PAIS_ORIGEN' column values\n",
    "if \"97\" in df[\"PAIS_ORIGEN\"].values:\n",
    "    df[\"PAIS_ORIGEN\"] = 'No aplica'\n",
    "    df.to_json(json_file_path, orient=\"records\", lines=False, indent=4)\n",
    "    print(\"Valores de 'PAIS_ORIGEN' actualizados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Descripción: debido a que el df que utilizamos en el campo \"PAIS_ORIGEN\" solo cuenta con el id 97 que significa que no aplica vamos a sustituir el id con su valor en el mismo campo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modificar fechas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON file into a DataFrame\n",
    "df = pd.read_json(json_file_path, orient=\"records\", lines=False)\n",
    "\n",
    "# List of date columns\n",
    "date_columns = [\n",
    "    'FECHA_ACTUALIZACION',\n",
    "    'FECHA_INGRESO',\n",
    "    'FECHA_SINTOMAS',\n",
    "    'FECHA_DEF'\n",
    "]\n",
    "\n",
    "# Check and convert date format for each date column\n",
    "for date_column in date_columns:\n",
    "    if date_column in df.columns:\n",
    "        # Replace invalid dates with empty strings\n",
    "        if '9999-99-99' in df[date_column].values:\n",
    "            df[date_column] = df[date_column].replace('9999-99-99', '')\n",
    "\n",
    "        if not df[date_column].str.match(r'\\d{2}-\\d{2}-\\d{4}').all():\n",
    "            # Convert the date format\n",
    "            df[date_column] = pd.to_datetime(df[date_column], errors='coerce').dt.strftime('%m-%d-%Y')\n",
    "\n",
    "            # Save the modified DataFrame back to JSON\n",
    "            df.to_json(json_file_path, orient=\"records\", lines=False, indent=4)\n",
    "            print(f\"Formato de fecha actualizado para la columna {date_column}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Descripción: como se solicita tenemos que cambiar el formato de las fechas de df entonces primeramente identificamos que hay 4 campos que son de tipo fecha entonces vamos a crear un arreglo para iterar y modificar estos campos 1 por 1 y se hizo una modificación que debido a que elasticSearch no acepta fechas en formato de 99-99-9999 que indican que el paciente no tiene esta fecha pues vamos a cambiarlo por un campo vació\n",
    "\n",
    "### Modificar resultado de laboratorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON file into a DataFrame\n",
    "df = pd.read_json(json_file_path, orient=\"records\", lines=False)\n",
    "\n",
    "# Check and convert 'RESULTADO_LAB' column values\n",
    "if df[\"RESULTADO_LAB\"].isin([1, 2, 3, 4, 97]).any():\n",
    "    df[\"RESULTADO_LAB\"] = df[\"RESULTADO_LAB\"].replace({\n",
    "        1: 'Positivo', \n",
    "        2: 'Negativo',\n",
    "        3: 'Pendiente',\n",
    "        4: 'Resultado no aplicable',\n",
    "        97: 'No se realizó la prueba'\n",
    "        })\n",
    "\n",
    "    # Save the modified DataFrame back to JSON\n",
    "    df.to_json(json_file_path, orient=\"records\", lines=False, indent=4)\n",
    "    print(\"Valores de 'RESULTADO_LAB' actualizados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Descripción: simplemente remplazamos los id´s por su valores correspondientes\n",
    "\n",
    "### Modificar clasificación final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON file into a DataFrame\n",
    "df = pd.read_json(json_file_path, orient=\"records\", lines=False)\n",
    "\n",
    "# Check and convert 'CLASIFICACION_FINAL' column values\n",
    "if df[\"CLASIFICACION_FINAL\"].isin([1, 2, 3, 4, 5, 6, 7]).any():\n",
    "    df[\"CLASIFICACION_FINAL\"] = df[\"CLASIFICACION_FINAL\"].replace({\n",
    "        1: 'CASO DE COVID-19 CONFIRMADO POR ASOCIACIÓN CLÍNICA EPIDEMIOLÓGICA',\n",
    "        2: 'CASO DE COVID-19 CONFIRMADO POR COMITÉ DE DICTAMINACIÓN',\n",
    "        3: 'CASO DE SARS-COV-2 CONFIRMADO',\n",
    "        4: 'INVÁLIDO POR LABORATORIO',\n",
    "        5: 'NO REALIZADO POR LABORATORIO',\n",
    "        6: 'CASO SOSPECHOSO',\n",
    "        7: 'NEGATIVO A SARS-COV-2',\n",
    "        })\n",
    "\n",
    "    # Save the modified DataFrame back to JSON\n",
    "    df.to_json(json_file_path, orient=\"records\", lines=False, indent=4)\n",
    "    print(\"Valores de 'CLASIFICACION_FINAL' actualizados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Descripción: simplemente remplazamos los id´s por su valores correspondientes\n",
    "\n",
    "### Modificar sexo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON file into a DataFrame\n",
    "df = pd.read_json(json_file_path, orient=\"records\", lines=False)\n",
    "\n",
    "# Check and convert 'SEXO' column values\n",
    "if df[\"SEXO\"].isin([1, 2, 99]).any():\n",
    "    df[\"SEXO\"] = df[\"SEXO\"].replace({2: 'Hombre', 1: 'Mujer', 99: 'No Especificado'})\n",
    "\n",
    "    # Save the modified DataFrame back to JSON\n",
    "    df.to_json(json_file_path, orient=\"records\", lines=False, indent=4)\n",
    "    print(\"Valores de 'SEXO' actualizados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Descripción: simplemente remplazamos los id´s por su valores correspondientes\n",
    "\n",
    "### Modificar le tipo de paciente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON file into a DataFrame\n",
    "df = pd.read_json(json_file_path, orient=\"records\", lines=False)\n",
    "\n",
    "# Check and convert 'TIPO_PACIENTE' column values\n",
    "if df[\"TIPO_PACIENTE\"].isin([1, 2, 99]).any():\n",
    "    df[\"TIPO_PACIENTE\"] = df[\"TIPO_PACIENTE\"].replace({1: 'Ambulatorio', 2: 'Hospitalizado', 99: 'No Especificado'})\n",
    "\n",
    "    # Save the modified DataFrame back to JSON\n",
    "    df.to_json(json_file_path, orient=\"records\", lines=False, indent=4)\n",
    "    print(\"Valores de 'TIPO_PACIENTE' actualizados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Descripción: simplemente remplazamos los id´s por su valores correspondientes\n",
    "\n",
    "### Modificaciones de origen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON file into a DataFrame\n",
    "df = pd.read_json(json_file_path, orient=\"records\", lines=False)\n",
    "\n",
    "# Check and convert 'ORIGEN' column values\n",
    "if df[\"ORIGEN\"].isin([1, 2, 99]).any():\n",
    "    df[\"ORIGEN\"] = df[\"ORIGEN\"].replace({1: 'USMER', 2: 'Fuera de USMER', 99: 'No Especificado'})\n",
    "\n",
    "    # Save the modified DataFrame back to JSON\n",
    "    df.to_json(json_file_path, orient=\"records\", lines=False, indent=4)\n",
    "    print(\"Valores de 'ORIGEN' actualizados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Descripción: simplemente remplazamos los id´s por su valores correspondientes\n",
    "\n",
    "### Modificaciones de sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON file into a DataFrame\n",
    "df = pd.read_json(json_file_path, orient=\"records\", lines=False)\n",
    "\n",
    "# Check and convert 'SECTOR' column values\n",
    "if df[\"SECTOR\"].isin([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 99]).any():\n",
    "    df[\"SECTOR\"] = df[\"SECTOR\"].replace({\n",
    "        1: 'CRUZ ROJA',\n",
    "        2: 'DIF',\n",
    "        3: 'ESTATAL',\n",
    "        4: 'IMSS',\n",
    "        5: 'IMSS-BIENESTAR',\n",
    "        6: 'ISSSTE',\n",
    "        7: 'MUNICIPAL',\n",
    "        8: 'PEMEX',\n",
    "        9: 'PRIVADA',\n",
    "        10: 'SEDENA',\n",
    "        11: 'SEMAR',\n",
    "        12: 'SSA',\n",
    "        13: 'UNIVERSITARIO',\n",
    "        14: 'CIJ',\n",
    "        15: 'IMSS Bienestar OPD',\n",
    "        99: 'No Especificado'\n",
    "    })\n",
    "\n",
    "    # Save the modified DataFrame back to JSON\n",
    "    df.to_json(json_file_path, orient=\"records\", lines=False, indent=4)\n",
    "    print(\"Valores de 'SECTOR' actualizados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Descripción: simplemente remplazamos los id´s por su valores correspondientes\n",
    "\n",
    "### Modificaciones de nacionalidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON file into a DataFrame\n",
    "df = pd.read_json(json_file_path, orient=\"records\", lines=False)\n",
    "\n",
    "# Check and convert 'Nacionalidad' column values\n",
    "if df[\"NACIONALIDAD\"].isin([1, 2, 99]).any():\n",
    "    df[\"NACIONALIDAD\"] = df[\"NACIONALIDAD\"].replace({1: 'Mexicana', 2: 'Extranjera', 99: 'No Especificado'})\n",
    "\n",
    "    # Save the modified DataFrame back to JSON\n",
    "    df.to_json(json_file_path, orient=\"records\", lines=False, indent=4)\n",
    "    print(\"Valores de 'NACIONALIDAD' actualizados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Descripción: simplemente remplazamos los id´s por su valores correspondientes\n",
    "\n",
    "### Modificaciones de columnas estándar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON file into a DataFrame\n",
    "df = pd.read_json(json_file_path, orient=\"records\", lines=False)\n",
    "\n",
    "Columnas_estándar = [\n",
    "    \"INTUBADO\",\n",
    "    \"NEUMONIA\",\n",
    "    \"EMBARAZO\",\n",
    "    \"HABLA_LENGUA_INDIG\",\n",
    "    \"INDIGENA\",\n",
    "    \"DIABETES\",\n",
    "    \"EPOC\",\n",
    "    \"ASMA\",\n",
    "    \"INMUSUPR\",\n",
    "    \"HIPERTENSION\",\n",
    "    \"OTRA_COM\",\n",
    "    \"CARDIOVASCULAR\",\n",
    "    \"OBESIDAD\",\n",
    "    \"RENAL_CRONICA\",\n",
    "    \"TABAQUISMO\",\n",
    "    \"OTRO_CASO\",\n",
    "    \"TOMA_MUESTRA_LAB\",\n",
    "    \"TOMA_MUESTRA_ANTIGENO\",\n",
    "    \"MIGRANTE\",\n",
    "    \"UCI\"\n",
    "]\n",
    "\n",
    "for columna in Columnas_estándar:\n",
    "    if df[columna].isin([1, 2, 97, 98, 99]).any():\n",
    "        df[columna] = df[columna].replace({1: 'Sí', 2: 'No', 97: 'No aplica', 98: 'Se ignora', 99: 'No especificado'})\n",
    "\n",
    "        # Save the modified DataFrame back to JSON\n",
    "        df.to_json(json_file_path, orient=\"records\", lines=False, indent=4)\n",
    "        print(f\"Valores de '{columna}' actualizados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Descripción: simplemente remplazamos los id´s por su valores correspondientes\n",
    "\n",
    "### Resultado\n",
    "\n",
    "Como resultado vamos a obtener un archivo json con un formato como el del siguiente ejemplo:\n",
    "\n",
    "```Json\n",
    "[\n",
    "    {\n",
    "        \"FECHA_ACTUALIZACION\":\"10-31-2021\",\n",
    "        \"ID_REGISTRO\":\"z4d6fe\",\n",
    "        \"ORIGEN\":\"USMER\",\n",
    "        \"SECTOR\":\"ISSSTE\",\n",
    "        \"ENTIDAD_UM\":{\n",
    "            \"ID\":24,\n",
    "            \"NOMBRE\":\"SAN LUIS POTOS\\u00cd\"\n",
    "        },\n",
    "        \"SEXO\":\"Mujer\",\n",
    "        \"ENTIDAD_NAC\":{\n",
    "            \"ID\":24,\n",
    "            \"NOMBRE\":\"SAN LUIS POTOS\\u00cd\"\n",
    "        },\n",
    "        \"TIPO_PACIENTE\":\"Ambulatorio\",\n",
    "        \"FECHA_INGRESO\":\"12-06-2020\",\n",
    "        \"FECHA_SINTOMAS\":\"11-30-2020\",\n",
    "        \"FECHA_DEF\":null,\n",
    "        \"INTUBADO\":\"No aplica\",\n",
    "        \"NEUMONIA\":\"No\",\n",
    "        \"EDAD\":55,\n",
    "        \"NACIONALIDAD\":\"Mexicana\",\n",
    "        \"EMBARAZO\":\"No\",\n",
    "        \"HABLA_LENGUA_INDIG\":\"No\",\n",
    "        \"INDIGENA\":\"No\",\n",
    "        \"DIABETES\":\"S\\u00ed\",\n",
    "        \"EPOC\":\"No\",\n",
    "        \"ASMA\":\"No\",\n",
    "        \"INMUSUPR\":\"No\",\n",
    "        \"HIPERTENSION\":\"No\",\n",
    "        \"OTRA_COM\":\"No\",\n",
    "        \"CARDIOVASCULAR\":\"No\",\n",
    "        \"OBESIDAD\":\"No\",\n",
    "        \"RENAL_CRONICA\":\"No\",\n",
    "        \"TABAQUISMO\":\"No\",\n",
    "        \"OTRO_CASO\":\"No\",\n",
    "        \"TOMA_MUESTRA_LAB\":\"S\\u00ed\",\n",
    "        \"RESULTADO_LAB\":\"Positivo\",\n",
    "        \"TOMA_MUESTRA_ANTIGENO\":\"No\",\n",
    "        \"RESULTADO_ANTIGENO\":\"No se realiz\\u00f3 la prueba\",\n",
    "        \"CLASIFICACION_FINAL\":\"CASO DE SARS-COV-2 CONFIRMADO\",\n",
    "        \"MIGRANTE\":\"No especificado\",\n",
    "        \"PAIS_NACIONALIDAD\":\"M\\u00e9xico\",\n",
    "        \"PAIS_ORIGEN\":\"No aplica\",\n",
    "        \"UCI\":\"No aplica\",\n",
    "        \"RES\":{\n",
    "            \"id_entidad\":24,\n",
    "            \"entidad\":\"SAN LUIS POTOS\\u00cd\",\n",
    "            \"id_municipio\":28,\n",
    "            \"municipio\":\"SAN LUIS POTOS\\u00cd\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "## Cargar los datos a Elastic Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de ya tener nuestro json en el formato deseado vamos a cargar estos datos a Elastic Search\n",
    "\n",
    "### Verificar conexión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "        # Create an instance of the ElasticSearchProvider class\n",
    "        # and establish a connection with the ElasticSearch server\n",
    "        es_handler = ElasticSearchProvider(index=\"covid-19\")\n",
    "        print(\"es_handler: \", es_handler, \"\\n\")\n",
    "        \n",
    "except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Descripción: Solo comprobamos la conexión con el constructor de la clase de ElasticSearchProvider() y le pasamos el nombre de nuestro indice\n",
    "\n",
    "Constructor de ElasticSearchProvider:\n",
    "\n",
    "```Python\n",
    "def __init__(self, index=\"person\"):\n",
    "    self.host = \"http://localhost:9200\"\n",
    "    #   self.user = str(user)\n",
    "    #   self.password = str(password)\n",
    "    self.index = index\n",
    "    self.index_type = \"_doc\"\n",
    "    # Configurar timeout, re-intentos y retry_on_timeout\n",
    "    self.connection = Elasticsearch(\n",
    "        self.host,\n",
    "        timeout=30,  # Tiempo de espera en segundos\n",
    "        max_retries=3,  # Número máximo de re-intentos\n",
    "        retry_on_timeout=True  # Re-intentar si hay un timeout\n",
    "    )\n",
    "```\n",
    "\n",
    "\n",
    "### Cargar el mapping de nuestro indice\n",
    "\n",
    "Covid-19_mapping.json:\n",
    "```Json\n",
    "{\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"FECHA_ACTUALIZACION\": { \"type\": \"date\", \"format\": \"MM-dd-yyyy\" },\n",
    "      \"ID_REGISTRO\": { \"type\": \"keyword\" },\n",
    "      \"ORIGEN\": { \"type\": \"keyword\" },\n",
    "      \"SECTOR\": { \"type\": \"keyword\" },\n",
    "      \"ENTIDAD_UM\": {\n",
    "        \"properties\": {\n",
    "          \"ID\": { \"type\": \"integer\" },\n",
    "          \"NOMBRE\": { \"type\": \"keyword\" }\n",
    "        }\n",
    "      },\n",
    "      \"SEXO\": { \"type\": \"keyword\" },\n",
    "      \"ENTIDAD_NAC\": {\n",
    "        \"properties\": {\n",
    "          \"ID\": { \"type\": \"integer\" },\n",
    "          \"NOMBRE\": { \"type\": \"keyword\" }\n",
    "        }\n",
    "      },\n",
    "      \"TIPO_PACIENTE\": { \"type\": \"keyword\" },\n",
    "      \"FECHA_INGRESO\": { \"type\": \"date\", \"format\": \"MM-dd-yyyy\" },\n",
    "      \"FECHA_SINTOMAS\": { \"type\": \"date\", \"format\": \"MM-dd-yyyy\" },\n",
    "      \"FECHA_DEF\": {\n",
    "        \"type\": \"date\",\n",
    "        \"format\": \"MM-dd-yyyy\",\n",
    "        \"null_value\": null\n",
    "      },\n",
    "      \"INTUBADO\": { \"type\": \"keyword\" },\n",
    "      \"NEUMONIA\": { \"type\": \"keyword\" },\n",
    "      \"EDAD\": { \"type\": \"integer\" },\n",
    "      \"NACIONALIDAD\": { \"type\": \"keyword\" },\n",
    "      \"EMBARAZO\": { \"type\": \"keyword\" },\n",
    "      \"HABLA_LENGUA_INDIG\": { \"type\": \"keyword\" },\n",
    "      \"INDIGENA\": { \"type\": \"keyword\" },\n",
    "      \"DIABETES\": { \"type\": \"keyword\" },\n",
    "      \"EPOC\": { \"type\": \"keyword\" },\n",
    "      \"ASMA\": { \"type\": \"keyword\" },\n",
    "      \"INMUSUPR\": { \"type\": \"keyword\" },\n",
    "      \"HIPERTENSION\": { \"type\": \"keyword\" },\n",
    "      \"OTRA_COM\": { \"type\": \"keyword\" },\n",
    "      \"CARDIOVASCULAR\": { \"type\": \"keyword\" },\n",
    "      \"OBESIDAD\": { \"type\": \"keyword\" },\n",
    "      \"RENAL_CRONICA\": { \"type\": \"keyword\" },\n",
    "      \"TABAQUISMO\": { \"type\": \"keyword\" },\n",
    "      \"OTRO_CASO\": { \"type\": \"keyword\" },\n",
    "      \"TOMA_MUESTRA_LAB\": { \"type\": \"keyword\" },\n",
    "      \"RESULTADO_LAB\": { \"type\": \"keyword\" },\n",
    "      \"TOMA_MUESTRA_ANTIGENO\": { \"type\": \"keyword\" },\n",
    "      \"RESULTADO_ANTIGENO\": { \"type\": \"keyword\" },\n",
    "      \"CLASIFICACION_FINAL\": { \"type\": \"keyword\" },\n",
    "      \"MIGRANTE\": { \"type\": \"keyword\" },\n",
    "      \"PAIS_NACIONALIDAD\": { \"type\": \"keyword\" },\n",
    "      \"PAIS_ORIGEN\": { \"type\": \"keyword\" },\n",
    "      \"UCI\": { \"type\": \"keyword\" },\n",
    "      \"RES\": {\n",
    "        \"properties\": {\n",
    "          \"id_entidad\": { \"type\": \"integer\" },\n",
    "          \"entidad\": { \"type\": \"keyword\" },\n",
    "          \"id_municipio\": { \"type\": \"integer\" },\n",
    "          \"municipio\": { \"type\": \"keyword\" }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with ElasticSearchProvider(index=\"covid-19\") as es:\n",
    "        \n",
    "        # Load the json of the mapping of the index into a dictionary\n",
    "        mapping_data = pd.read_json(\"./Proyecto Unidad I/Covid-19_mapping.json\")\n",
    "        mapping = mapping_data.to_dict()\n",
    "\n",
    "        response = es.create_index(mapping=mapping)\n",
    "        print(f\"{RESPONSE_LITERAL} {json.dumps(response.body if hasattr(response, 'body') else response, indent=4)}\\n\")\n",
    "\n",
    "        response = es.get_mapping()\n",
    "        print(f\"{RESPONSE_LITERAL} {json.dumps(response.body if hasattr(response, 'body') else response, indent=4)}\\n\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"an error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Descripción: con esto cargamos el mapping a nuestro indice de Elastic Search\n",
    "\n",
    "Función create_index(mapping) y get_mapping() de la clase ElasticSearchProvider:\n",
    "\n",
    "```Python\n",
    "def create_index(self, mapping):\n",
    "    try:\n",
    "        if not self.connection.indices.exists(index=self.index):\n",
    "            response = self.connection.indices.create(index=self.index, body=mapping)\n",
    "        else:\n",
    "            response = {\n",
    "                \"StatusCode\": 400,\n",
    "                \"body\": json.dumps({\n",
    "                    \"message\": f\"Index {self.index} already exists\"\n",
    "                })\n",
    "            }\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"StatusCode\": 500,\n",
    "            \"body\": json.dumps({\n",
    "                \"message\": str(e)\n",
    "                })\n",
    "        }\n",
    "\n",
    "def get_mapping(self):\n",
    "    try:\n",
    "        response = self.connection.indices.get_mapping(index=self.index)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"StatusCode\": 500,\n",
    "            \"body\": json.dumps({\n",
    "                \"message\": str(e)\n",
    "                })\n",
    "        }\n",
    "```\n",
    "\n",
    "### Cargar los datos a nuestro indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with ElasticSearchProvider(index=\"covid-19\") as es:\n",
    "        \n",
    "        response = es.load_json_file(json_file_path, batch_size=10000)\n",
    "        print(f\"{RESPONSE_LITERAL} {json.dumps(response.body if hasattr(response, 'body') else response, indent=4)}\\n\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"an error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Descripción: Con esta función cargamos en bloques de 10,000 los datos a nuestro servidor de Elastic Search\n",
    "\n",
    "Función load_json_file(json_file_path, batch_size) de ElasticSearchProvider:\n",
    "```Python\n",
    "def load_json_file(self, file_path, batch_size=1000):\n",
    "    try:\n",
    "        # Leer el archivo JSON usando pandas\n",
    "        df = pd.read_json(file_path, orient=\"records\", lines=False)\n",
    "\n",
    "        # Convertir el DataFrame a una lista de diccionarios\n",
    "        documents = df.to_dict(orient=\"records\")\n",
    "        \n",
    "        # Dividir los documentos en lotes\n",
    "        for i in range(0, len(documents), batch_size):\n",
    "            batch = documents[i:i + batch_size]\n",
    "            \n",
    "            # Preparar los datos para la operación bulk\n",
    "            bulk_data = [\n",
    "                {\"_index\": self.index, \"_source\": doc}\n",
    "                for doc in batch\n",
    "            ]\n",
    "            \n",
    "            # Insertar los documentos en Elasticsearch\n",
    "            helpers.bulk(self.connection, bulk_data)\n",
    "        \n",
    "        return f\"{len(documents)} documents inserted in {self.index}\"\n",
    "    \n",
    "    except ValueError as e:\n",
    "        return {\n",
    "            \"StatusCode\": 400,\n",
    "            \"body\": json.dumps({\n",
    "                \"message\": f\"Error reading JSON with pandas: {str(e)}\"\n",
    "            })\n",
    "        }\n",
    "    except FileNotFoundError as e:\n",
    "        return {\n",
    "            \"StatusCode\": 404,\n",
    "            \"body\": json.dumps({\n",
    "                \"message\": f\"File not found: {str(e)}\"\n",
    "            })\n",
    "        }\n",
    "    except ConnectionError as e:\n",
    "        return {\n",
    "            \"StatusCode\": 500,\n",
    "            \"body\": json.dumps({\n",
    "                \"message\": f\"Connection error: {str(e)}\"\n",
    "            })\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"StatusCode\": 500,\n",
    "            \"body\": json.dumps({\n",
    "                \"message\": f\"An error occurred: {str(e)}\"\n",
    "            })\n",
    "        }\n",
    "```\n",
    "\n",
    "### Comprobar que se subió exitosamente y una consulta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with ElasticSearchProvider(index=\"covid-19\") as es:\n",
    "        \n",
    "        # Search for documents in the index\n",
    "        print(\"Search Covid-19 Documents Response:\")\n",
    "        response = es.get_all_documents()\n",
    "        print(f\"{RESPONSE_LITERAL} {json.dumps(response.body if hasattr(response, 'body') else response, indent=4)}\\n\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"an error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Descripción: con esta función obtenemos de nuestro indice todos los registros o al menos los primeros 10,000 debido a que Elastic Search no te muestra mas de 10,000 registros a la vez \n",
    "\n",
    "Función get_all_documents() de ElasticSearchProvider:\n",
    "\n",
    "```Python\n",
    "def get_all_documents(self):\n",
    "    try:\n",
    "        response = self.connection.search(index=self.index, body={\"query\": {\"match_all\": {}}})\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"StatusCode\": 500,\n",
    "            \"body\": json.dumps({\n",
    "                \"message\": str(e)\n",
    "                })\n",
    "        }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with ElasticSearchProvider(index=\"covid-19\") as es:\n",
    "        \n",
    "        size = 32\n",
    "        response = es.search_document(query={\n",
    "            \"size\": 0,\n",
    "            \"aggs\": {\n",
    "                \"entidades\": {\n",
    "                    \"terms\": {\n",
    "                        \"field\": \"ENTIDAD_UM.NOMBRE\",\n",
    "                        \"size\": size\n",
    "                    },\n",
    "                    \"aggs\": {\n",
    "                        \"nombre_entidad\": {\n",
    "                            \"top_hits\": {\n",
    "                                \"size\": 1,\n",
    "                                \"_source\": {\n",
    "                                    \"includes\": [\"ENTIDAD_UM.ID\"]\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        })\n",
    "\n",
    "        # Process the response to extract data for the DataFrame\n",
    "        if hasattr(response, 'body'):\n",
    "            response_data = response.body\n",
    "        else:\n",
    "            response_data = response\n",
    "        \n",
    "        # Extract buckets from the aggregation\n",
    "        buckets = response_data.get(\"aggregations\", {}).get(\"entidades\", {}).get(\"buckets\", [])\n",
    "        data = []\n",
    "        for bucket in buckets:\n",
    "            entidad_nombre = bucket.get(\"key\")\n",
    "            entidad_id = bucket.get(\"nombre_entidad\", {}).get(\"hits\", {}).get(\"hits\", [{}])[0].get(\"_source\", {}).get(\"ENTIDAD_UM.ID\")\n",
    "            data.append({\"Entidad Nombre\": entidad_nombre, \"Entidad ID\": entidad_id})\n",
    "\n",
    "        # Create a pandas DataFrame\n",
    "        df = pd.DataFrame(data)\n",
    "        print(\"DataFrame created from search_document response:\")\n",
    "        print(df)\n",
    "\n",
    "        print(f\"{RESPONSE_LITERAL} {json.dumps(response_data, indent=4)}\\n\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"an error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Descripción: Con esto obtenemos la cantidad de registros que tiene cada entidad junto con el id de la entidad\n",
    "\n",
    "Explicación del query:\n",
    "\n",
    "   1. Tamaño del resultado (size: 0): Indica que no se quieren recuperar documentos como tal, sino que se busca obtener únicamente los resultados de las agregaciones.\n",
    "\n",
    "   2. Agregaciones (aggs): Se usan para obtener información agrupada de los datos.\n",
    "\n",
    "```Json\n",
    "    \"size\": 0,\n",
    "    \"aggs\": {\n",
    "        \"entidades\": {\n",
    "            \"terms\": {\n",
    "                \"field\": \"ENTIDAD_UM.NOMBRE\",\n",
    "                \"size\": size\n",
    "            },\n",
    "            \"aggs\": {\n",
    "                \"nombre_entidad\": {\n",
    "                    \"top_hits\": {\n",
    "                        \"size\": 1,\n",
    "                        \"_source\": {\n",
    "                            \"includes\": [\"ENTIDAD_UM.ID\"]\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "```\n",
    "\n",
    "   1. Agrupación por ENTIDAD_UM.NOMBRE\n",
    "      - Se usa la agregación terms sobre el campo \"ENTIDAD_UM.NOMBRE\", lo que significa que se agruparán los documentos según el nombre de la entidad de unidad médica (ENTIDAD_UM).\n",
    "\n",
    "      - El parámetro \"size\": size define el número máximo de grupos a devolver. Este valor debe estar definido en la variable size.\n",
    "   2. Subagregación top_hits\n",
    "      - Una vez que se han agrupado los documentos por nombre de entidad, se agrega una subagregación top_hits, que selecciona hasta un documento por grupo (\"size\": 1).\n",
    "\n",
    "      - Solo se extrae el campo \"ENTIDAD_UM.ID\", es decir, el identificador de la entidad de unidad médica correspondiente al nombre."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
